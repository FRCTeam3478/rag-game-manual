{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2024 Google LLC."
      ],
      "metadata": {
        "id": "Tce3stUlHN0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-04T18:22:49.680672Z",
          "iopub.execute_input": "2025-01-04T18:22:49.680995Z",
          "iopub.status.idle": "2025-01-04T18:22:49.687129Z",
          "shell.execute_reply.started": "2025-01-04T18:22:49.680961Z",
          "shell.execute_reply": "2025-01-04T18:22:49.685903Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Bot for FRC 2025 Game Manual\n",
        "\n",
        "Agent that answers questions about the FRC 2025 Game Manual\n",
        "\n",
        "## What is RAG?\n",
        "\n",
        "Two big limitations of LLMs are 1) that they only \"know\" the information that they were trained on, and 2) that they have limited input context windows. A way to address both of these limitations is to use a technique called Retrieval Augmented Generation, or RAG. A RAG system has three stages:\n",
        "\n",
        "1. Indexing\n",
        "2. Retrieval\n",
        "3. Generation\n",
        "\n",
        "Indexing happens ahead of time, and allows you to quickly look up relevant information at query-time. When a query comes in, you retrieve relevant documents, combine them with your instructions and the user's query, and have the LLM generate a tailored answer in natural language using the supplied information. This allows you to provide information that the model hasn't seen before, such as product-specific knowledge or live weather updates.\n",
        "\n",
        "In this notebook you will use the Gemini API to create a vector database, retrieve answers to questions from the database and generate a final answer. You will use [Chroma](https://docs.trychroma.com/), an open-source vector database. With Chroma, you can store embeddings alongside metadata, embed documents and queries, and search your documents."
      ],
      "metadata": {
        "id": "CsVPnR8VbXE6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "First, install ChromaDB and the Gemini API Python SDK."
      ],
      "metadata": {
        "id": "akuOzK4dJl3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U -q \"google-generativeai>=0.8.3\" chromadb\n",
        "%pip install httpx PyPDF2"
      ],
      "metadata": {
        "id": "JbXe7Oodc5dP",
        "execution": {
          "iopub.status.busy": "2025-01-04T18:22:49.688498Z",
          "iopub.execute_input": "2025-01-04T18:22:49.688814Z",
          "iopub.status.idle": "2025-01-04T18:23:11.490438Z",
          "shell.execute_reply.started": "2025-01-04T18:22:49.688784Z",
          "shell.execute_reply": "2025-01-04T18:23:11.488862Z"
        },
        "trusted": true,
        "outputId": "27437a92-6069-4960-e3af-358fe6ec9bcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m605.5/605.5 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx) (1.2.2)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may see output containing `ERROR: pip's dependency resolver does not currently take into account all the packages that are installed` - this is OK, the packages are still installed and compatible for this codelab. Also note that you do not have to restart the kernel."
      ],
      "metadata": {
        "id": "q8NwuYJFtsjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from IPython.display import Markdown"
      ],
      "metadata": {
        "id": "muuhsDmmKdHi",
        "execution": {
          "iopub.status.busy": "2025-01-04T18:23:29.165291Z",
          "iopub.execute_input": "2025-01-04T18:23:29.16577Z",
          "iopub.status.idle": "2025-01-04T18:23:29.762481Z",
          "shell.execute_reply.started": "2025-01-04T18:23:29.165725Z",
          "shell.execute_reply": "2025-01-04T18:23:29.761373Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up your API key\n",
        "\n",
        "To run the following cell, your API key must be stored it in a [Kaggle secret](https://www.kaggle.com/discussions/product-feedback/114053) named `GOOGLE_API_KEY`.\n",
        "\n",
        "If you don't already have an API key, you can grab one from [AI Studio](https://aistudio.google.com/app/apikey). You can find [detailed instructions in the docs](https://ai.google.dev/gemini-api/docs/api-key).\n",
        "\n",
        "To make the key available through Kaggle secrets, choose `Secrets` from the `Add-ons` menu and follow the instructions to add your key or enable it for this notebook."
      ],
      "metadata": {
        "id": "FQOGMejVu-6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "ysayz8skEfBW",
        "execution": {
          "iopub.status.busy": "2025-01-04T18:23:35.7781Z",
          "iopub.execute_input": "2025-01-04T18:23:35.778969Z",
          "iopub.status.idle": "2025-01-04T18:23:35.987032Z",
          "shell.execute_reply.started": "2025-01-04T18:23:35.778926Z",
          "shell.execute_reply": "2025-01-04T18:23:35.986068Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you received an error response along the lines of `No user secrets exist for kernel id ...`, then you need to add your API key via `Add-ons`, `Secrets` **and** enable it.\n",
        "\n",
        "![Screenshot of the checkbox to enable GOOGLE_API_KEY secret](https://storage.googleapis.com/kaggle-media/Images/5gdai_sc_3.png)"
      ],
      "metadata": {
        "id": "52b101760a45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore available models\n",
        "\n",
        "You will be using the [`embedContent`](https://ai.google.dev/api/embeddings#method:-models.embedcontent) API method to calculate embeddings in this guide. Find a model that supports it through the [`models.list`](https://ai.google.dev/api/models#method:-models.list) endpoint. You can also find more information about the embedding models on [the models page](https://ai.google.dev/gemini-api/docs/models/gemini#text-embedding).\n",
        "\n",
        "`text-embedding-004` is the most recent embedding model, so you will use it for this exercise."
      ],
      "metadata": {
        "id": "fegnGFpMS4AI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for m in genai.list_models():\n",
        "    if \"embedContent\" in m.supported_generation_methods:\n",
        "        print(m.name)"
      ],
      "metadata": {
        "id": "Km5d13_FS2Q_",
        "execution": {
          "iopub.status.busy": "2025-01-04T18:23:39.541418Z",
          "iopub.execute_input": "2025-01-04T18:23:39.541808Z",
          "iopub.status.idle": "2025-01-04T18:23:39.678701Z",
          "shell.execute_reply.started": "2025-01-04T18:23:39.541774Z",
          "shell.execute_reply": "2025-01-04T18:23:39.67727Z"
        },
        "trusted": true,
        "outputId": "e10c59c6-8907-4482-ba1f-3ec6ca22c87e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/embedding-001\n",
            "models/text-embedding-004\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data\n",
        "\n",
        "Extraction of the data of the game manual from the PDF available online"
      ],
      "metadata": {
        "id": "3XWKXoXwOGxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import httpx\n",
        "import base64\n",
        "from io import BytesIO\n",
        "from PyPDF2 import PdfReader\n",
        "import re\n",
        "\n",
        "def extract_sections(pdf_url):\n",
        "    try:\n",
        "        response = httpx.get(pdf_url)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "    except httpx.HTTPError as e:\n",
        "        print(f\"Error downloading PDF: {e}\")\n",
        "        return []\n",
        "\n",
        "    pdf_bytes = BytesIO(response.content)\n",
        "    reader = PdfReader(pdf_bytes)\n",
        "\n",
        "    sections = []\n",
        "    for page in reader.pages:\n",
        "        text = page.extract_text()\n",
        "        if(len(text.strip()) > 5): # Ignore blank pages\n",
        "            sections.append(text.strip())\n",
        "\n",
        "    return sections\n",
        "\n",
        "doc_url = \"https://firstfrc.blob.core.windows.net/frc2025/Manual/2025GameManual.pdf\"\n",
        "pages = extract_sections(doc_url)\n",
        "\n",
        "if pages:\n",
        "    print(len(pages))\n",
        "else:\n",
        "    print(\"No sections found or error occurred.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-04T19:02:16.832547Z",
          "iopub.execute_input": "2025-01-04T19:02:16.832927Z",
          "iopub.status.idle": "2025-01-04T19:02:25.351012Z",
          "shell.execute_reply.started": "2025-01-04T19:02:16.832892Z",
          "shell.execute_reply": "2025-01-04T19:02:25.349633Z"
        },
        "id": "8nMLwpv8tsjf",
        "outputId": "8a0da972-9bb7-412c-f593-e86cd5c826c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "150\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the embedding database with ChromaDB\n",
        "\n",
        "Create a [custom function](https://docs.trychroma.com/guides/embeddings#custom-embedding-functions) to generate embeddings with the Gemini API. In this task, you are implementing a retrieval system, so the `task_type` for generating the *document* embeddings is `retrieval_document`. Later, you will use `retrieval_query` for the *query* embeddings. Check out the [API reference](https://ai.google.dev/api/embeddings#v1beta.TaskType) for the full list of supported tasks.\n"
      ],
      "metadata": {
        "id": "yDzxArLeOexD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
        "from google.api_core import retry\n",
        "\n",
        "\n",
        "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
        "    # Specify whether to generate embeddings for documents, or queries\n",
        "    document_mode = True\n",
        "\n",
        "    def __call__(self, input: Documents) -> Embeddings:\n",
        "        if self.document_mode:\n",
        "            embedding_task = \"retrieval_document\"\n",
        "        else:\n",
        "            embedding_task = \"retrieval_query\"\n",
        "\n",
        "        retry_policy = {\"retry\": retry.Retry(predicate=retry.if_transient_error)}\n",
        "\n",
        "        response = genai.embed_content(\n",
        "            model=\"models/text-embedding-004\",\n",
        "            content=input,\n",
        "            task_type=embedding_task,\n",
        "            request_options=retry_policy,\n",
        "        )\n",
        "        return response[\"embedding\"]"
      ],
      "metadata": {
        "id": "mF7Uu1kCQsT0",
        "execution": {
          "iopub.status.busy": "2025-01-04T19:02:25.352759Z",
          "iopub.execute_input": "2025-01-04T19:02:25.353087Z",
          "iopub.status.idle": "2025-01-04T19:02:25.360208Z",
          "shell.execute_reply.started": "2025-01-04T19:02:25.353055Z",
          "shell.execute_reply": "2025-01-04T19:02:25.359117Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now create a [Chroma database client](https://docs.trychroma.com/getting-started) that uses the `GeminiEmbeddingFunction` and populate the database with the documents you defined above."
      ],
      "metadata": {
        "id": "HrDWLyopPNBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "\n",
        "DB_NAME = \"frcmanual\"\n",
        "embed_fn = GeminiEmbeddingFunction()\n",
        "embed_fn.document_mode = True\n",
        "\n",
        "chroma_client = chromadb.Client()\n",
        "db = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn)\n",
        "\n",
        "db.add(documents=pages, ids=[str(i) for i in range(len(pages))])"
      ],
      "metadata": {
        "id": "OITXgxZlLoXU",
        "execution": {
          "iopub.status.busy": "2025-01-04T19:02:25.361765Z",
          "iopub.execute_input": "2025-01-04T19:02:25.36208Z",
          "iopub.status.idle": "2025-01-04T19:02:27.21998Z",
          "shell.execute_reply.started": "2025-01-04T19:02:25.362051Z",
          "shell.execute_reply": "2025-01-04T19:02:27.218355Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm that the data was inserted by looking at the database."
      ],
      "metadata": {
        "id": "2QbwFgfXp-fL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db.count()\n",
        "# You can peek at the data too.\n",
        "# db.peek(1)"
      ],
      "metadata": {
        "id": "kQ9PHUL_l-hf",
        "execution": {
          "iopub.status.busy": "2025-01-04T18:39:19.854743Z",
          "iopub.execute_input": "2025-01-04T18:39:19.855102Z",
          "iopub.status.idle": "2025-01-04T18:39:19.865332Z",
          "shell.execute_reply.started": "2025-01-04T18:39:19.855072Z",
          "shell.execute_reply": "2025-01-04T18:39:19.864369Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fc1b635-3eb6-49a8-df0c-6b4984bc27d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval: Find relevant documents\n",
        "\n",
        "To search the Chroma database, call the `query` method. Note that you also switch to the `retrieval_query` mode of embedding generation.\n"
      ],
      "metadata": {
        "id": "Tu5zRErgsQ8u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Augmented generation: Answer the question\n",
        "\n",
        "Now that you have found a relevant passage from the set of documents (the *retrieval* step), you can now assemble a generation prompt to have the Gemini API *generate* a final answer. Note that in this example only a single passage was retrieved. In practice, especially when the size of your underlying data is large, you will want to retrieve more than one result and let the Gemini model determine what passages are relevant in answering the question. For this reason it's OK if some retrieved passages are not directly related to the question - this generation step should ignore them."
      ],
      "metadata": {
        "id": "s8PNRMpOQkm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch to query mode when generating embeddings.\n",
        "embed_fn.document_mode = False\n",
        "\n",
        "# Search the Chroma DB using the specified query.\n",
        "query = \"Weight limit of the robot\"\n",
        "\n",
        "result = db.query(query_texts=[query], n_results=1)\n",
        "[[passage]] = result[\"documents\"]\n",
        "\n",
        "Markdown(passage)\n",
        "\n",
        "passage_oneline = passage.replace(\"\\n\", \" \")\n",
        "query_oneline = query.replace(\"\\n\", \" \")\n",
        "\n",
        "# This prompt is where you can specify any guidance on tone, or what topics the model should stick to, or avoid.\n",
        "prompt = f\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below.\n",
        "Be sure to respond in a complete sentence, being comprehensive, including all relevant background information.\n",
        "Be sure to break down complicated concepts and\n",
        "strike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.\n",
        "\n",
        "QUESTION: {query_oneline}\n",
        "PASSAGE: {passage_oneline}\n",
        "\"\"\"\n",
        "#print(prompt)\n",
        "\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash-latest\")\n",
        "answer = model.generate_content(prompt)\n",
        "Markdown(answer.text)"
      ],
      "metadata": {
        "id": "gQdJMbTSLtKE",
        "execution": {
          "iopub.status.busy": "2025-01-04T19:02:39.02811Z",
          "iopub.execute_input": "2025-01-04T19:02:39.028516Z",
          "iopub.status.idle": "2025-01-04T19:02:40.106967Z",
          "shell.execute_reply.started": "2025-01-04T19:02:39.028472Z",
          "shell.execute_reply": "2025-01-04T19:02:40.105787Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "69f452e1-7916-439a-d1d5-cd6c5cac6b3b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The maximum weight allowed for a robot is 115 lbs (approximately 52 kg).  This includes the basic robot structure and all mechanisms used in a single configuration.  However, bumpers, the robot battery and associated cable (up to 12 inches per leg), and location detection tags (if provided by the event) are excluded from the weight calculation.\n"
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Passage in which was obtained the information: {passage}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSMlY0QGwfiR",
        "outputId": "e17baec4-9de4-4ce0-c561-b5805d463910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passage in which was obtained the information: Section 6 Game Details  V0 47 of 160 Figure 6-5 Examples of CORAL scored in the trough (L1)  \n",
            " \n",
            "Figure 6-5 includes examples of CORAL  on the REEF at the conclusion of a \n",
            "MATCH.  \n",
            "A. CORAL  A, C, F, and H  are scored because they meet criteria A. \n",
            "B. CORAL B  and G are scored because they meet criteria B . \n",
            "C. CORAL D and E do not meet any criteria for trough scoring.  \n",
            " \n",
            "A CORAL is scored  on L2-L4 BRANCH  if the end of the BRANCH  is inside the volume of the CORAL and the \n",
            "CORAL  is not in contact with a ROBOT or a n ALGAE . \n",
            "An ALGAE  is scored in a PROCESSOR  once it has passed through the opening of the PROCESSOR  and by the \n",
            "sensor array.  An ALGAE  is scored in a NET  if it is above the NET and within the perimeter of the NET . \n",
            "If a CORAL scored in AUTO gets removed from a BRANCH during TELEOP, the AUTO points are removed. If a \n",
            "CORAL is scored in that location again, the AUTO points associated with the original scored CORAL is restored.  \n",
            "CORAL scored in the trough is not tracked by specific location, if a CORAL is removed from the trough after \n",
            "AUTO, the points removed will correspond to the lowest scoring CORAL (i.e. TELEOP CORAL removed first) ; if \n",
            "CORAL is re -scored in the trough, points will be re -added in the reverse order (i.e. AUTO  CORAL re -added first).  \n",
            "6.5.2  ROBOT  Scoring Criteria  \n",
            "To qualify for LEAVE points, a ROBOT  must move such that its BUMPERS no longer overlap its  ROBOT \n",
            "STARTING LINE  at the end of AUTO.  \n",
            "To qualify for PARK points, a ROBOT ’S BUMPERS  must  be partially or  completely contained in the ir BARGE \n",
            "ZONE  at the end of the MATCH  and does not meet the criteria for CAGE  points . \n",
            "To qualify for CAGE  points, a ROBOT  must be contacting a CAGE (with the exception of the ANCHOR ), not \n",
            "contacting the carpet,  and may additionally contact only the following elements : \n",
            "A. SCORING ELEMENT S,\n"
          ]
        }
      ]
    }
  ]
}